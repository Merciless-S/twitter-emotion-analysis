{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import emoji\n",
    "from Model import Model\n",
    "import torch\n",
    "from torch import nn\n",
    "import nltk\n",
    "import collections\n",
    "#read the data for tweet analysis\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nevade': 'Nevada', 'presbyterian': 'Presbyterian', 'rsx': 'RSX', 'Steffen': 'Stephen', 'susan': 'Susan', 'abilitey': 'ability', 'abouy': 'about', 'absorbtion': 'absorption', 'accidently': 'accidentally', 'accomodate': 'accommodate', 'acommadate': 'accommodate', 'acord': 'accord', 'aquantance': 'acquaintance', 'equire': 'acquire', 'adultry': 'adultery', 'aggresive': 'aggressive', 'alchohol': 'alcohol', 'alchoholic': 'alcoholic', 'allieve': 'alive', 'alright': 'all_right'}\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"input/text_emotion.csv\")\n",
    "\n",
    "#read the misspelled and corresponding correction of the data\n",
    "misspell_data = pd.read_csv(\"input/aspell.txt\",sep=\":\",names=[\"correction\",\"misspell\"])\n",
    "\n",
    "#import contraction dataset\n",
    "contractions = pd.read_csv(\"input/contractions.csv\")\n",
    "cont_dic = dict(zip(contractions.Contraction, contractions.Meaning))\n",
    "\n",
    "#remove white space on two sides\n",
    "misspell_data.misspell = misspell_data.misspell.str.strip()\n",
    "\n",
    "#split all possible misspelled data of a word in a array\n",
    "misspell_data.misspell = misspell_data.misspell.str.split(\" \")\n",
    "\n",
    "#expand the column in misspell from array to many rows\n",
    "misspell_data = misspell_data.explode(\"misspell\").reset_index(drop=True)\n",
    "\n",
    "#remove duplicates\n",
    "misspell_data.drop_duplicates(\"misspell\",inplace=True)\n",
    "\n",
    "#create a dic object only for print purpose\n",
    "miss_corr = dict(zip(misspell_data.misspell, misspell_data.correction))\n",
    "print({v:miss_corr[v] for v in [list(miss_corr.keys())[k] for k in range(20)]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean data complete\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "this function replace contraction to by actual meaning\n",
    "'''\n",
    "def cont_to_meaning(val):\n",
    "    for x in val.split():\n",
    "        if x in cont_dic.keys():\n",
    "            val = val.replace(x, cont_dic[x])\n",
    "    return val\n",
    "\n",
    "'''\n",
    "this function replace misspelled words to the correct one\n",
    "'''\n",
    "def misspelled_correction(val):\n",
    "    for x in val.split():\n",
    "        if x in miss_corr.keys():\n",
    "            val = val.replace(x, miss_corr[x])\n",
    "    return val\n",
    "\n",
    "'''\n",
    "this function remove all punctuation\n",
    "'''\n",
    "def punctuation(val):\n",
    "    punctuations = '''()-[]{};:'\"\\,<>./@#$%^&_~'''\n",
    "\n",
    "    for x in val.lower():\n",
    "        if x in punctuations:\n",
    "            val = val.replace(x, \" \")\n",
    "    return val\n",
    "\n",
    "'''\n",
    "This function is the combination of three functions above\n",
    "'''\n",
    "def clean_text(val):\n",
    "    val = misspelled_correction(val)\n",
    "    val = cont_to_meaning(val)\n",
    "    #val = p.clean(val)\n",
    "    val = ' '.join(punctuation(emoji.demojize(val)).split())\n",
    "    return val\n",
    "\n",
    "\n",
    "data[\"clean_content\"] = data.content.apply(lambda x : clean_text(x))\n",
    "print(\"clean data complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train and test data split complete\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "prepare to map emotions to integer \n",
    "'''\n",
    "#map each emotion to a integer\n",
    "sent_to_id  = {\"empty\":0, \"sadness\":1,\"enthusiasm\":2,\"neutral\":3,\"worry\":4,\"surprise\":5,\"love\":6,\"fun\":7,\"hate\":8,\"happiness\":9,\"boredom\":10,\"relief\":11,\"anger\":12}\n",
    "data[\"sentiment_id\"] = data['sentiment'].map(sent_to_id)\n",
    "\n",
    "#convert the Y to matrix from categorical variable\n",
    "Y = np.ndarray((len(data),len(sent_to_id)),dtype=float)\n",
    "#Y = [np.zeros(len(sent_to_id), dtype = int) for _ in  range(len(data))]\n",
    "for i in range(len(data)):\n",
    "    Y[i][data.sentiment_id[i]] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.clean_content,Y, random_state= 2021, test_size=0.2, shuffle=True)\n",
    "\n",
    "print(\"train and test data split complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize complete\n"
     ]
    }
   ],
   "source": [
    "sentences = X_train.tolist()\n",
    "X_train = [None for _ in range(len(sentences))]\n",
    "uniquewords = collections.Counter()\n",
    "\n",
    "#Record all unique non-trivial words in training data\n",
    "for s in sentences:\n",
    "    tokens = nltk.word_tokenize(s)\n",
    "    for w in tokens:\n",
    "        uniquewords[w] += 1\n",
    "word_bag = list(uniquewords.keys())\n",
    "\n",
    "#tokenize each sentence in training data\n",
    "for j,s in enumerate(sentences):\n",
    "    tokens = nltk.word_tokenize(s)\n",
    "    bag = np.zeros(len(word_bag), dtype=np.float32)\n",
    "    for i in range(len(word_bag)):\n",
    "        if word_bag[i] in tokens:\n",
    "            bag[i] = 1\n",
    "    X_train[j] = bag\n",
    "X_train = np.array(X_train)\n",
    "\n",
    "print(\"Tokenize complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_samples = len(X_train)\n",
    "        self.x_data = X_train\n",
    "        self.y_data = y_train\n",
    "\n",
    "    # support indexing such that dataset[i] can be used to get i-th sample\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    # we can call len(dataset) to return the size\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepare to train\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.0001\n",
    "epochs = 200\n",
    "batch_size = 200\n",
    "num_epochs = 200\n",
    "hidden_size = 50\n",
    "\n",
    "\n",
    "correct_train = total_train = 0\n",
    "\n",
    "dataset = ChatDataset()\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "input_size = len(X_train[0])\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "net = Model(input_size, hidden_size, len(sent_to_id))\n",
    "CUDA = torch.cuda.is_available()\n",
    "if CUDA:\n",
    "    net = net.cuda()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = learning_rate)\n",
    "print(\"prepare to train\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (words, labels) in enumerate(train_loader):\n",
    "        words = words.cuda()\n",
    "        labels = labels.to(dtype=torch.long).cuda()\n",
    "        outputs = net(words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
